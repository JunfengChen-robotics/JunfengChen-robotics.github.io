<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <title>Junfeng Chen</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/png" href="./imgs/photo.jpg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 1 -->
        <!-- Âø´Êç∑ÂØºËà™ÊåâÈíÆÂå∫Âüü -->
        <div style="width:100%; text-align:center; margin-bottom: 10px;">
            <a href="#news-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">News</a>
            <a href="#publications-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">Publications</a>
            <a href="#honors-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">Honors</a>
            <a href="#service-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">Service</a>
            <a href="#aboutme-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">About Me</a>
            <a href="#activity-section" style="display:inline-block; margin:0 10px; padding:6px 16px; background:#f5f5f5; border-radius:6px; color:#1772d0; font-weight:bold; font-size:15px; border:1px solid #e0e0e0; transition:background 0.2s;">Activity Highlights</a>
        </div>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody><tr>
                            <td width="68%" valign="middle">
                                <p align="center"><name>Junfeng (Eric) Chen (Èôà‰øäÈîã)</name></p>
                    I am currently a Ph.D. candidate majoring in Robotics Engineering, <a href="https://amr.pku.edu.cn/">AMR</a>
                    at <a href="https://english.pku.edu.cn/">PKU</a>,
                    supervised by <a href="https://mengguo.github.io/personal_site/"> Prof. Meng Guo</a> and <a href="https://www.zhongkuili-pku.com/home/"> Prof. Zhongkui Li</a>.
                    Previously, I was a researcher in  <a href="https://airs.cuhk.edu.cn/en/team/tinlunlam">Intelligent Robot Center</a>,
                    at <a href="https://airs.cuhk.edu.cn/en">ShenZhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</a>, 
                    supervised by <a href="https://sites.google.com/site/lamtinlun"> Prof. Lam Tinlun</a> from 2020.05-2022.04.

                    </br></br>
                    
                    My research focuses on <b>autonomous, intelligent, safe, reliable, and interpretable task planning</b> for 
                    heterogeneous multi-robot systems. I leverage interdisciplinary methods from artificial intelligence 
                    (LLMs, reinforcement learning, graph neural networks), optimization theory, robotics (SLAM, planning, control), and systems engineering 
                    (distributed systems, edge computing). 

                    </br></br>

                    During my Ph.D., I have mainly explored four directions:
                    <ul>
                    <li><b>Scalable distributed task planning</b> for large-scale and highly dynamic system.</li>
                    <li><b>Hybrid optimization and game-theoretic approaches</b> for dynamic task planning under competition and cooperation.</li>
                    <li><b>LLM-based and human-in-the-loop task planning</b> for unknown and open-world scenarios.</li>
                    <li><b>Communication-aware coordination</b> for efficient and robust multi-robot collaboration.</li>
                    </ul>

                    In addition, I have developed several heterogeneous multi-robot platforms, combining both hardware and software, 
                    to validate and advance the practical impact of my theoretical research.
                    Please visit my <a href="https://www.youtube.com/channel/UCkTl9bSxyO4-h6QsztvT6Ew">YouTube channel</a> and <a href="bilibili channel">BiliBili channel</a> for videos.

                    </br></br>

	            </br>
                </p><p align="center">
                    <a href="mailto:chenjunfeng[-at-]stu.pku.edu.cn">Email</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=PorL6uYAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> /
                    <a href="https://github.com/JunfengChen-robotics"> Github </a> 
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 170; height: 225;"></td></tr>
            </tbody>
          </table>
    
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
            <td>
            <heading>Pinned</heading>
            <p align="justify">
            <b>‚úâÔ∏è</b> <em><span style="color: red; font-weight: bold;">I'm expected to graduate in 2026 and open to both academia and industry positions. If you're interested, please feel free to contact me.</span></em> </p>
            <b>üìå</b> <em><span style="color: black; font-weight: bold;">I am seeking self-motivated undergraduate and graduate students for academic collaboration, 
            for future research at the intersection of <u>VLA, LLMs, and world models.</u></span></em>
            </td></tr>
        </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="news-section">
        <tbody><tr>
            <td>
            <heading>News</heading>
            <p align="justify">
            <li><b>[09/2025]</b> Invited talk at <a href="http://www.ase.buaa.edu.cn/">Beihang University</a> on the field of intelligent multi-robot systems.</li>
            <li><b>[07/2025]</b> Invited talk at <a href="https://www.tmslab.cn/en/">TIANMUSHAN LABORATORY</a> on the field of LLM-enhanced multi-robot systems.</li>
            <li><b>[06/2025]</b> One paper was accepted to IROS.</li> 
            <li><b>[05/2024]</b> One paper was accepted to ICRA.</li> 
            <li><b>[12/2023]</b> One paper was accepted to ROBIO.</li> 
            <li><b>[09/2023]</b> Invited talk at <a href="https://www.afeu.edu.cn/index.htm"> Air Force Engineering University </a> on the field of game-theoretic catching systems.</li>
            <li><b>[11/2023]</b> One paper was accepted to RA-L,selected as <strong style="color: red;">Oral</strong>.</li> 
            <li><b>[05/2023]</b> One paper was accepted to CDC.</li>
            <li><b>[04/2023]</b> One paper was accepted to TR-O,selected as <strong style="color: red;">Oral</strong>.</li>
            <li><b>[05/2022]</b> Two papers were accepted to ICRA.</li>
            <li><b>[09/2021]</b> Two papers were accepted to IROS.</li>
            <li><b>[02/2021]</b> Two papers were accepted to RA-L.</li>
            <li><b>[02/2020]</b> Two papers were accepted to Computer Communications.</li>
            </p>
            </td></tr>
        </tbody>
        </table>


    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="publications-section">
        <tbody><tr>
          <td><heading>Publications</heading>
          </td>
	</tr></tbody>
    </table>
    
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
			<br> (<sup>#</sup> for the corresponding author, * for equal contribution)


        <td width="20%"><img src="./imgs/LLM-SR.png" alt="PontTuset" width="250" height="180" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.science.org/journal/scirobotics">
                <papertitle>LLMs Meet Formal Methods for Robot Swarms: Reliable, Explainable and Efficient Human-in-the-loop Planning in Unknown Environments</papertitle></a> 
                <br> <strong>Junfeng Chen</strong>, Yuxiao Zhu, An Zhuo, Xintong Zhang, Shuo Zhang, Meng Guo<sup>#</sup>, and Zhongkui Li<sup>#</sup>.
                <br> <em>Submitted to Science Robotics (<strong>SR</strong>), 2025.</em>
                <br>
                <!-- <a href="https://zxt1234567.github.io/CoCoPlan-HomePage/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a> -->
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose a formal method and LLM framework for coordinating large fleets of heterogeneous robots in open and dynamic environments. Our approach integrates model-checking-based task planning with LLM-powered reasoning and interaction, ensuring adaptability, explainability, and optimal mission execution. Validated through simulations and real-world deployments, it proves effective for disaster response, infrastructure inspection, and dynamic surveillance.</p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/ambush.png" alt="PontTuset" width="250" height="140" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/t-ro/submission-procedures">
                <papertitle>AMBUSH: Collaborative Capture in Complex Environments with Neural Acceleration</papertitle></a> 
                <br> <strong>Junfeng Chen</strong>, Yinhang Luo, Junrui Li, Xinyi Wang, and Meng Guo<sup>#</sup>.
                <br> <em>Submitted to IEEE Transactions on Robotics (<strong>TR-O</strong>), 2025.</em>
                <br>
                <a href="https://zxt1234567.github.io/CoCoPlan-HomePage/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>H-MCTS Ambush</strong>, a parameterized ambush strategy optimized with Hybrid Monte Carlo Tree Search for collaborative capture of dynamic targets. Our method accounts for topology, visibility, speed ratios, and capture ranges, while leveraging learned heuristics to accelerate planning without sacrificing quality, successfully capturing faster and even human-controlled evaders in complex environments.</p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/cocoplan.png" alt="PontTuset" width="250" height="180" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/ra-l/submission-procedures">
                <papertitle>CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments </papertitle></a>
                <br> <strong></strong>Xintong Zhang*, <strong>Junfeng Chen*</strong>, Yuxiao Zhu, and Meng Guo<sup>#</sup>.
                <br> <em>Submitted to IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2025</em>
                <br>
                <a href="https://zxt1234567.github.io/CoCoPlan-HomePage/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>CoCoPlan</strong>, a unified framework that co-optimizes collaborative task planning and intermittent communication for multi-robot systems. 
                    Our approach integrates the branch-and-bound task encoding, adaptive efficiency objectives, and optimized event scheduling to handle dynamic environments under limited connectivity in both office and disaster-response scenarios.</p>
            </td>
        </tr> 


        <td width="20%"><img src="./imgs/slei3d.png" alt="PontTuset" width="250" height="180" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/t-ase/information-for-authors-t-ase">
                <papertitle>SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication </papertitle></a>
                <br> <strong>Junfeng Chen</strong>, Yuxiao Zhu, Xintong Zhang, and Meng Guo<sup>#</sup>.
                <br> <em>Revision on IEEE Transactions on Automation Science and Engineering (<strong>T-ASE</strong>), 2025</em>
                <br>
                <a href="https://junfengchen-robotics.github.io/SLEI3D/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>SLEI3D</strong>, a planning and coordination framework for heterogeneous multi-robot systems to perform simultaneous 3D exploration, inspection, and real-time reporting in unknown environments. Our approach integrates adaptive inspection and intermittent communication protocols with a multi-layer, multi-rate planning mechanism for robust coordination.</p>
            </td>
        </tr> 


        <td width="20%"><img src="./imgs/dexter.png" alt="PontTuset" width="250" height="140" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.iros25.org/">
                <papertitle>DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models </papertitle></a>
                <br> Yuxiao Zhu*, <strong>Junfeng Chen*</strong>, Xintong Zhang, Meng Guo, and Zhongkui Li<sup>#</sup>.
                <br> <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025</em>
                <br>
                <a href="https://arxiv.org/pdf/2508.14387"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://tcxm.github.io/DEXTER-LLM/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>DEXTER-LLM</strong>, a novel framework for dynamic task planning in unknown environments. Our approach integrates LLM-based multi-stage reasoning, optimization-based task assignment, and adaptive human-in-the-loop verification to tackle the challenges of online adaptability and explainability.</p>
            </td>
        </tr> 
        
        <td width="20%"><img src="./imgs/meta.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://2024.ieee-icra.org/">
                <papertitle>Meta-reinforcement learning based cooperative surface inspection of 3d uncertain structures using multi-robot systems </papertitle></a>
                <br> <strong>Junfeng Chen</strong>, Yuan Gao, Junjie Hu, Fuqin Deng, and Tin Lun Lam<sup>#</sup> </a>.
                <br> <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2024.</em>
                <br>
                <a href="https://arxiv.org/pdf/2109.13617"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://ieeexplore.ieee.org/abstract/document/10610420"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <!-- <a class="github-button" href="https://github.com/HKUST-Aerial-Robotics/FALCON" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star SYSU-STAR/FC-Hetero on GitHub">Star</a> -->
                <a href="https://youtu.be/6RMGOIROfj8"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>Meta-Scan</strong>, a meta-learning approach for efficient sensor scanning in 3D and uncertain environments using heterogeneous multi-robot systems. Our method enhances exploration and rapid adaptation in the complex scenarios. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/multirobotlearn.png" alt="PontTuset" width="250" height="200" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="http://www.hlab.sys.es.osaka-u.ac.jp/people/wan/robio2023/Home.html">
                <papertitle>Multirobolearn: An open-source framework for multi-robot deep reinforcement learning </papertitle></a>
                <br> <strong>Junfeng Chen</strong>, Fuqing Deng, Yuan Gao, Junjie Hu, Xiyue Guo, Guanqi Liang, and Tin Lun Lam<sup>#</sup>.
                <br> <em>IEEE International Conference on Robotics and Biomimetics (<strong>ROBIO</strong>), 2023. </em>
                <!-- <br> <em> <strong style="color: red;">Oral</strong>  (<strong style="color: red;">acceptance rate: 10%</strong>)</em> -->
                <br>
                <!-- <a href="https://arxiv.org/abs/2409.02738"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b"> -->
                <a href="https://ieeexplore.ieee.org/abstract/document/10354600"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">   
                <a class="github-button" href="https://github.com/JunfengChen-robotics/MultiRoboLearn" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star JunfengChen-robotics/MultiRoboLearn on GitHub">Star</a>
                <a href="https://youtu.be/i43FQOe0PMI"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>MultiRoboLearn</strong>, an open-source framework that bridges multi-agent deep reinforcement learning with real-world multi-robot applications. Our framework offers standardized, easy-to-use simulation scenarios seamlessly transferable to physical multi-robot systems, and provides a benchmark for varying algorithm comparisons.</p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/kss.png" alt="PontTuset" width="250" height="120" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/ra-l/submission-procedures">
                <papertitle>Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource Defense </papertitle></a>
                <br> <strong>Junfeng Chen</strong>, Zili Tang, and Meng Guo<sup>#</sup></a>.
                <br> <em>IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2023. </em>
                <br>
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=PorL6uYAAAAJ&view_op=list_works&sortby=pubdate"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://youtu.be/sxYdGW2zbp8"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>K-Serial Coalition</strong>, a distributed strategy for adaptive coalition formation in large-scale heterogeneous multi-robot systems. Our approach combines a K-serial stable coalition algorithm with a heterogeneous graph attention network heuristic to enhance online adaptability while guaranteeing solution quality.</p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/cdc.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://cdc2023.ieeecss.org/index.html#">
                <papertitle>Combinatorial-hybrid Optimization for Multi-agent Systems under Collaborative Tasks </papertitle></a>
                <br> Zili Tang, <strong>Junfeng Chen</strong>*, and Meng Guo<sup>#</sup>.
                <br> <em>IEEE Conference on Decision and Control (<strong>CDC</strong>), 2023.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/10384264"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>CHO-Framework</strong>, a combinatorial-hybrid optimization approach for multi-agent coordination. Our method jointly addresses coalition formation and collaborative control by interleaving discrete task assignment with continuous behavior optimization to guarantee the feasibility and quality.</p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/tro.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/t-ro/submission-procedures">
                <papertitle>Asymmetric self-play-enabled intelligent heterogeneous multirobot catching system using deep multiagent reinforcement learning</papertitle></a> 
                <br> Yuangao, <strong>Junfeng Chen</strong>, Xi Chen, Chongyang Wang, Junjie Hu, Fuqin Deng, and Tin Lun Lam<sup>#</sup>.
                <br> <em>IEEE Transactions on Robotics (<strong>TR-O</strong>), 2023.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/10101687"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://youtu.be/cF04em7ASwY"> <img src="https://img.shields.io/badge/YouTube-Video-blue"></a>
                
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>ASP-CL Framework</strong>, an actor-critic multi-agent reinforcement learning approach that integrates asymmetric self-play and curriculum learning for heterogeneous multi-robot systems. Our method enables cooperative behaviors in adversarial catching tasks under real-world constraints.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/energy.png" alt="PontTuset" width="250" height="120" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://samueli.ucla.edu/upcoming-events/virtual-access-to-the-2022-international-conference-on-robotics-and-automation-icra/">
                <papertitle>Energy sharing mechanism for a freeform robotic system-freebot</papertitle></a>
                <br> Guanqi Liang, Yuxiao Tu, Lijun Zong, <strong>Junfeng Chen</strong>, and Tin Lun Lam<sup>#</sup>.
                <br> <em> IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2022. </em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9811860"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b"></a>
                <!-- <a href="https://www.bilibili.com/video/BV1Fr421j7oC/?spm_id_from=333.999.0.0&vd_source=0af61c122e5e37c944053b57e313025a"><img alt="Talk" src="https://img.shields.io/badge/BiliBili-Talk-purple"/> -->


                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>FreeBOT-Energy</strong>, an energy sharing mechanism for modular self-reconfigurable robots that enables modules to exchange power through surface contact. Our approach designs robust sharing rules and network structures to ensure sustainability and maximize participating modules.</p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/acoustic.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
	            <p><a href="https://iros2022.org/">
	            <papertitle>AcousticFusion: Fusing sound source localization to visual SLAM in dynamic environments</papertitle></a>
                <br> Tianwei Zhang, Huayan Zhang, Xiaofei Li, <strong>Junfeng Chen</strong>, Tin Lun Lam, and Sethu Vijayakumar<sup>#</sup>.
                <br> <em> IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2021.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9636585"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://www.youtube.com/watch?v=8eNikzp9LIQ"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>AV-SLAM</strong>, an audio-visual fusion approach that integrates sound source direction with RGB-D sensing to handle dynamic obstacles in multi-robot SLAM. Our method replaces costly object detectors with lightweight DoA-based cues, enabling efficient on-board processing and achieving stable localization across diverse dynamic environments.</p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/semantic.png" alt="PontTuset" width="250" height="200" style="border-style: none"></td>
        <td width="80%" valign="top">
	            <p><a href="https://www.ieee-ras.org/publications/ra-l">
	            <papertitle>Semantic histogram based graph matching for real-time multi-robot global localization in large scale environment</papertitle></a>
                <br> Xiyue Guo, Junjie Hu, <strong>Junfeng Chen</strong>, Fuqin Deng, and Tin Lun Lam<sup>#</sup>.
                <br> <em> IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2021.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9353207"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://www.youtube.com/watch?v=xB8WHj8K9cE"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>SH-Graph</strong>, a semantic histogram-based graph matching method for efficient and robust multi-robot global localization. Our approach overcomes large viewpoint variations while enabling real-time performance in both homogeneous and heterogeneous robot systems.</p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/depth.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
	            <p><a href="https://www.ieee-ras.org/publications/ra-l">
	            <papertitle>A two-stage unsupervised approach for low light image enhancement</papertitle></a>
                <br>  Junjie Hu, Xiyue Guo, <strong>Junfeng Chen</strong>, Guanqi, Liang, Fuqin Deng, and Tin Lun Lam<sup>#</sup>.
                <br> <em> IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2021.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9312402/"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://www.youtube.com/watch?v=3l0lLd3FFPY"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>ULIE-Net</strong>, a two-stage unsupervised framework for low-light image enhancement. Our method combines Retinex-based pre-enhancement with an adversarially trained refinement network, overcoming the need for paired data, poor dark-scene performance, and noise amplification.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/mission.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0140366419313684">
	            <papertitle>A novel mission planning method for UAVs‚Äô course of action</papertitle></a>
                <br> Yaoming Zhou, Haoran Zhao, <strong>Junfeng Chen</strong>, and Yuhong Jia<sup>#</sup>.
                <br> <em> Computer Communication, 2020.</em>
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0140366419313684"> <img src="https://img.shields.io/badge/Elsevier-PDF-orange"> </a>
                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>NSG-Planner</strong>, a mission planning method for UAV courses of action based on a two-segment nested scheme generation strategy. Our approach integrates task decomposition and resource scheduling to automatically produce diverse operational schemes with superior task completion time to verify its effectiveness and flexibility.</p>
            </td>
        </tr>
        

    </tbody>
    </table> 

    <!--SECTION 4 -->
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="honors-section">
         <tbody><tr>
             <td>
             <heading>Honors & Awards</heading>
          <p><b>[08/2024]</b>, Postgraduate Studentship.</p>
          <p><b>[09/2018]</b>, First-class Graduate studentship.</p>
          <p><b>[09/2017]</b>, First-class Graduate studentship.</p>
          <p><b>[11/2015]</b>, National Scholarship - Ministry of Education, PRC.</p>
          <p><b>[11/2014]</b>, National Scholarship - Ministry of Education, PRC.</p>
		   </td></tr>
       </tbody>
    </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="service-section">
        <tbody><tr>
           <td>
           <heading>Service</heading>
               <p> <strong>Reviewer</strong>: <em>T-RO, RA-L, T-ASE, JFR, IET Cyber-Systems and Robotics, ICRA, IROS</em></p>
               <p> <strong>Teaching Assistant</strong>: <li><a href=" https://github.com/JunfengChen-robotics/hybrid_control_match">Hybrid Control Course </a>(2023,2024,2025 Fall) </li> 
                <li>Graduate Academic Writing (2024 Spring)</li></p>
            </td></tr>
        </tbody>
     </table>

    <!--SECTION 6 -->
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="aboutme-section">
         <tbody><tr>
             <td>
             <heading>About Me</heading>
              <p> <strong>Skills</strong>: Python, C++, Matlab, C#, ROS, Machine Learning (ML), RL, Graph Learning, Git & GitHub, Robotics Simulator (Gazebo, Isaac Gym, Unity, V-rep, Mujoco, PyBullet), CATIA, AutoCAD, Robotic Platforms (Tello, Crazyflies, Quadrotor, TurtleBot, KKSwarms, Scout, AgileX, Go2), Heterogeneous Multi-Robot Systems.</p>
		   </td></tr>
       </tbody>
    </table>


            <!-- Activity Highlights Section -->

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="background: linear-gradient(0deg, #fcf8f8 0%, #feffff 100%); margin-top: 30px; border-radius: 18px;" id="activity-section">
                        <tbody><tr>
                    <td>
                    <heading>Activity Highlights</heading>
                    </td></tr>
            <tr>
                <td colspan="4" align="center" style="padding-bottom: 0;">
                    <p style="color:#444; font-size:1.1em; margin-bottom: 10px;">A glimpse of our recent experimental activities and achievements</p>
                </td>
            </tr>
            <tr>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/catch.webp" alt="Activity 1" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">Epic Moment: Large-scale Robot Catching in Action</div>
                    <div style="color: #666; font-size: 0.98em;">Dozens of robots working in perfect harmony‚Äîthis is the pinnacle of collective intelligence, captured in a single breathtaking moment.</div>
                </td>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/hetero.webp" alt="Activity 2" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">Heterogeneous Robots: Each Shines in Their Own Way</div>
                    <div style="color: #666; font-size: 0.98em;">Multiple types of robots, each with unique capabilities, collaborate seamlessly to showcase the beauty of diverse intelligent teamwork.</div>
                </td>
            </tr>
            <tr>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/group_Photo_LLM.webp" alt="Activity 3" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">LLM Team: The Ultimate Group Photo</div>
                    <div style="color: #666; font-size: 0.98em;">Our team, united by passion and innovation‚Äîthis photo captures the spirit of AI and human synergy at its finest.</div>
                </td>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/group_photo_CocoPlan.webp" alt="Activity 4" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">CocoPlan: Behind the Scenes of the Release</div>
                    <div style="color: #666; font-size: 0.98em;">Witness the creative spark and teamwork that power our cutting-edge algorithms‚Äîevery breakthrough has a story.</div>
                </td>
            </tr>
            <tr>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/group_photo_SLEI3D.webp" alt="Activity 5" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">SLEI3D: The Art of 3D Robot Collaboration</div>
                    <div style="color: #666; font-size: 0.98em;">Robots collaborating in 3D space‚Äîwhere technology meets aesthetics, and innovation becomes art.</div>
                </td>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/hybrid.jpg" alt="Activity 6" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">Ambush: A Multi-Agent System Showcase</div>
                    <div style="color: #666; font-size: 0.98em;">Intelligent Game System: Multi-agent strategic competition and cooperation in complex environments.</div>
                </td>
            </tr>
            <tr>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/competion.webp" alt="Activity 7" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;"> Competition Highlight: The Ultimate Showdown</div>
                    <div style="color: #666; font-size: 0.98em;">Intense competition, rapid strategies, and the thrill of the challenge‚Äîthis is where intelligent agents push their limits.</div>
                </td>
                <td width="50%" align="center" valign="top">
                    <img src="imgs/total.webp" alt="Activity 8" width="300" height="160" style="border-radius: 12px; box-shadow: 0 2px 8px #e0e7ef;">
                    <div style="font-weight: bold; font-size: 1.1em; color: #1772d0; margin-top: 10px;">Hybrid Control Course Competition Highlight</div>
                    <div style="color: #666; font-size: 0.98em;">A memorable group photo from the graduate-level Hybrid Control course competition‚Äîshowcasing the excitement, teamwork, and achievements of all participants.</div>
                </td>
            </tr>
        </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
            <td width="100%" align="middle">
            <p align="center" style="width: 25% ">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=xkVDQxg8nSSD4gBPK_6qDjaYSleekdvNsK5tLhNM_xk"></script>
            </p></td>
        </tr>
        </tbody>
    </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
		       <p align="right"><font size="2"> Last update: 2025.09.14. <a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>
            </td>
         </tr>
         </tbody>
     </table>




</td>
</tr>
</tbody>
</table>
</body>
</html>
